import os
import subprocess
import hashlib
import json
import glob
from dotenv import load_dotenv
from openai import OpenAI

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_PATH = os.path.join(BASE_DIR, "..", "scraper", "data")
STATE_FILE = os.path.join(BASE_DIR, "pipeline_state.json")
ASSISTANT_ID = os.getenv("ASSISTANT_ID")

load_dotenv(dotenv_path=os.path.join(BASE_DIR, "..", ".env"))
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

def get_file_hash(filepath):
    hasher = hashlib.md5()
    with open(filepath, 'rb') as f:
        hasher.update(f.read())
    return hasher.hexdigest()

def load_state():
    if os.path.exists(STATE_FILE):
        with open(STATE_FILE, 'r') as f:
            return json.load(f)
    return {}

def run_daily_job():
    print("--- [1/3] Running Node.js Scraper ---")
    # Chạy file JS của bạn bằng Node.js
    scraper_path = os.path.join(BASE_DIR, "..", "scraper", "src", "index.js")
    subprocess.run(["node", scraper_path], check=True)

    print("\n--- [2/3] Detecting Delta Changes ---")
    old_state = load_state()
    new_state = {}
    
    files_to_upload = []
    added, updated, skipped = 0, 0, 0

    # Scan for .md files generated by the scraper
    md_files = glob.glob(os.path.join(DATA_PATH, "*.md"))
    
    for filepath in md_files:
        filename = os.path.basename(filepath)
        current_hash = get_file_hash(filepath)
        new_state[filename] = current_hash

        if filename not in old_state:
            added += 1
            files_to_upload.append(filepath)
        elif old_state[filename] != current_hash:
            updated += 1
            files_to_upload.append(filepath)
        else:
            skipped += 1

    print(f"Status: Added: {added}, Updated: {updated}, Skipped: {skipped}")

    if not files_to_upload:
        print("No changes detected. Skipping OpenAI upload.")
        return

    print(f"\n--- [3/3] Uploading Delta ({len(files_to_upload)} files) to OpenAI ---")
    try:
        # 1. Create a new Vector Store
        vector_store = client.vector_stores.create(name=f"OptiSigns_Delta_{added+updated}")
        
        # 2. Upload the changed files
        file_streams = [open(path, "rb") for path in files_to_upload]
        file_batch = client.vector_stores.file_batches.upload_and_poll(
            vector_store_id=vector_store.id, 
            files=file_streams
        )
        
        for f in file_streams: f.close()

        # 3. Update Assistant
        if ASSISTANT_ID:
            client.beta.assistants.update(
                assistant_id=ASSISTANT_ID,
                tool_resources={"file_search": {"vector_store_ids": [vector_store.id]}},
            )
            print(f"Assistant updated successfully.")

        with open(STATE_FILE, 'w') as f:
            json.dump(new_state, f)
            
        print(f"Job completed. Embedded: {file_batch.file_counts}")

    except Exception as e:
        print(f"Error during OpenAI sync: {e}")

if __name__ == "__main__":
    run_daily_job()